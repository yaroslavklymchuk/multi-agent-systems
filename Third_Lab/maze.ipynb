{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maze_2d.maze_2d_q_learning import simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_action_proba = 1\n",
    "np.random.choice([True, False], p=[make_action_proba, 1 - make_action_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([True, False], p=[make_action_proba, 1 - make_action_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array((10, 10)) - np.array((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym_maze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ab40b1fd0eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym_maze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaze_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym_maze'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym_maze.envs import maze_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"maze-random-12x12-plus-v0\")\n",
    "env = maze_env.MazeEnv(maze_size=(12, 12), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Defining the environment related constants\n",
    "'''\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "MAZE_SIZE = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "NUM_BUCKETS = MAZE_SIZE  # one bucket per grid\n",
    "\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n  # [\"N\", \"S\", \"E\", \"W\"]\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "\n",
    "'''\n",
    "Learning related constants\n",
    "'''\n",
    "MIN_EXPLORE_RATE = 0.001\n",
    "MIN_LEARNING_RATE = 0.2\n",
    "DECAY_FACTOR = np.prod(MAZE_SIZE, dtype=float) / 10.0\n",
    "\n",
    "'''\n",
    "Defining the simulation related constants\n",
    "'''\n",
    "NUM_EPISODES = 50000\n",
    "MAX_T = np.prod(MAZE_SIZE, dtype=int) * 100\n",
    "STREAK_TO_END = 100\n",
    "SOLVED_T = np.prod(MAZE_SIZE, dtype=int)\n",
    "DEBUG_MODE = 1\n",
    "RENDER_MAZE = True\n",
    "ENABLE_RECORDING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(env, q_table, state, explore_rate):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Select the action with the highest q\n",
    "    else:\n",
    "        action = int(np.argmax(q_table[state]))\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(0.8, 1.0 - math.log10((t+1)/DECAY_FACTOR)))\n",
    "\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.8, 1.0 - math.log10((t+1)/DECAY_FACTOR)))\n",
    "\n",
    "\n",
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(q_table, env):\n",
    "\n",
    "    # Instantiating the learning related parameters\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "    discount_factor = 0.99\n",
    "\n",
    "    num_streaks = 0\n",
    "\n",
    "    # Render tha maze\n",
    "    env.render()\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        state_0 = state_to_bucket(obv)\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(MAX_T):\n",
    "\n",
    "            # Select an action\n",
    "            action = select_action(env, q_table, state_0, explore_rate)\n",
    "\n",
    "            # execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            state = state_to_bucket(obv)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update the Q based on the result\n",
    "            best_q = np.amax(q_table[state])\n",
    "            q_table[state_0 + (action,)] += learning_rate * (reward + discount_factor * (best_q) - q_table[state_0 + (action,)])\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state_0 = state\n",
    "\n",
    "            # Print data\n",
    "            if DEBUG_MODE == 2:\n",
    "                print(\"\\nEpisode = %d\" % episode)\n",
    "                print(\"t = %d\" % t)\n",
    "                print(\"Action: %d\" % action)\n",
    "                print(\"State: %s\" % str(state))\n",
    "                print(\"Reward: %f\" % reward)\n",
    "                print(\"Best Q: %f\" % best_q)\n",
    "                print(\"Explore rate: %f\" % explore_rate)\n",
    "                print(\"Learning rate: %f\" % learning_rate)\n",
    "                print(\"Streaks: %d\" % num_streaks)\n",
    "                print(\"\")\n",
    "\n",
    "            elif DEBUG_MODE == 1:\n",
    "                if done or t >= MAX_T - 1:\n",
    "                    print(\"\\nEpisode = %d\" % episode)\n",
    "                    print(\"t = %d\" % t)\n",
    "                    print(\"Explore rate: %f\" % explore_rate)\n",
    "                    print(\"Learning rate: %f\" % learning_rate)\n",
    "                    print(\"Streaks: %d\" % num_streaks)\n",
    "                    print(\"Total reward: %f\" % total_reward)\n",
    "                    print(\"\")\n",
    "\n",
    "            # Render tha maze\n",
    "            if RENDER_MAZE:\n",
    "                env.render()\n",
    "\n",
    "            if env.is_game_over():\n",
    "                sys.exit()\n",
    "\n",
    "            if done:\n",
    "                print(\"Episode %d finished after %f time steps with total reward = %f (streak %d).\"\n",
    "                      % (episode, t, total_reward, num_streaks))\n",
    "\n",
    "                if t <= SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "\n",
    "            elif t >= MAX_T - 1:\n",
    "                print(\"Episode %d timed out at %d with total reward = %f.\"\n",
    "                      % (episode, t, total_reward))\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_folder = \"/home/klimchuk/projects/TN/notebooks/WebPush/Multiagent/videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klimchuk/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = Monitor(env, recording_folder, video_callable=lambda episode: True,force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode = 0\n",
      "t = 2343\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: -0.627083\n",
      "\n",
      "Episode 0 finished after 2343.000000 time steps with total reward = -0.627083 (streak 0).\n",
      "\n",
      "Episode = 1\n",
      "t = 3105\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: -1.156250\n",
      "\n",
      "Episode 1 finished after 3105.000000 time steps with total reward = -1.156250 (streak 0).\n",
      "\n",
      "Episode = 2\n",
      "t = 1177\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.182639\n",
      "\n",
      "Episode 2 finished after 1177.000000 time steps with total reward = 0.182639 (streak 0).\n",
      "\n",
      "Episode = 3\n",
      "t = 2143\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: -0.488194\n",
      "\n",
      "Episode 3 finished after 2143.000000 time steps with total reward = -0.488194 (streak 0).\n",
      "\n",
      "Episode = 4\n",
      "t = 538\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.626389\n",
      "\n",
      "Episode 4 finished after 538.000000 time steps with total reward = 0.626389 (streak 0).\n",
      "\n",
      "Episode = 5\n",
      "t = 477\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.668750\n",
      "\n",
      "Episode 5 finished after 477.000000 time steps with total reward = 0.668750 (streak 0).\n",
      "\n",
      "Episode = 6\n",
      "t = 1322\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.081944\n",
      "\n",
      "Episode 6 finished after 1322.000000 time steps with total reward = 0.081944 (streak 0).\n",
      "\n",
      "Episode = 7\n",
      "t = 405\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.718750\n",
      "\n",
      "Episode 7 finished after 405.000000 time steps with total reward = 0.718750 (streak 0).\n",
      "\n",
      "Episode = 8\n",
      "t = 624\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.566667\n",
      "\n",
      "Episode 8 finished after 624.000000 time steps with total reward = 0.566667 (streak 0).\n",
      "\n",
      "Episode = 9\n",
      "t = 235\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.836806\n",
      "\n",
      "Episode 9 finished after 235.000000 time steps with total reward = 0.836806 (streak 0).\n",
      "\n",
      "Episode = 10\n",
      "t = 1795\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: -0.246528\n",
      "\n",
      "Episode 10 finished after 1795.000000 time steps with total reward = -0.246528 (streak 0).\n",
      "\n",
      "Episode = 11\n",
      "t = 523\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.636806\n",
      "\n",
      "Episode 11 finished after 523.000000 time steps with total reward = 0.636806 (streak 0).\n",
      "\n",
      "Episode = 12\n",
      "t = 1195\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.170139\n",
      "\n",
      "Episode 12 finished after 1195.000000 time steps with total reward = 0.170139 (streak 0).\n",
      "\n",
      "Episode = 13\n",
      "t = 905\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.371528\n",
      "\n",
      "Episode 13 finished after 905.000000 time steps with total reward = 0.371528 (streak 0).\n",
      "\n",
      "Episode = 14\n",
      "t = 316\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.780556\n",
      "\n",
      "Episode 14 finished after 316.000000 time steps with total reward = 0.780556 (streak 0).\n",
      "\n",
      "Episode = 15\n",
      "t = 399\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.722917\n",
      "\n",
      "Episode 15 finished after 399.000000 time steps with total reward = 0.722917 (streak 0).\n",
      "\n",
      "Episode = 16\n",
      "t = 353\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.754861\n",
      "\n",
      "Episode 16 finished after 353.000000 time steps with total reward = 0.754861 (streak 0).\n",
      "\n",
      "Episode = 17\n",
      "t = 368\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.744444\n",
      "\n",
      "Episode 17 finished after 368.000000 time steps with total reward = 0.744444 (streak 0).\n",
      "\n",
      "Episode = 18\n",
      "t = 459\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.681250\n",
      "\n",
      "Episode 18 finished after 459.000000 time steps with total reward = 0.681250 (streak 0).\n",
      "\n",
      "Episode = 19\n",
      "t = 140\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.902778\n",
      "\n",
      "Episode 19 finished after 140.000000 time steps with total reward = 0.902778 (streak 0).\n",
      "\n",
      "Episode = 20\n",
      "t = 282\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 1\n",
      "Total reward: 0.804167\n",
      "\n",
      "Episode 20 finished after 282.000000 time steps with total reward = 0.804167 (streak 1).\n",
      "\n",
      "Episode = 21\n",
      "t = 408\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.716667\n",
      "\n",
      "Episode 21 finished after 408.000000 time steps with total reward = 0.716667 (streak 0).\n",
      "\n",
      "Episode = 22\n",
      "t = 237\n",
      "Explore rate: 0.800000\n",
      "Learning rate: 0.800000\n",
      "Streaks: 0\n",
      "Total reward: 0.835417\n",
      "\n",
      "Episode 22 finished after 237.000000 time steps with total reward = 0.835417 (streak 0).\n",
      "\n",
      "Episode = 23\n",
      "t = 157\n",
      "Explore rate: 0.796635\n",
      "Learning rate: 0.796635\n",
      "Streaks: 0\n",
      "Total reward: 0.890972\n",
      "\n",
      "Episode 23 finished after 157.000000 time steps with total reward = 0.890972 (streak 0).\n",
      "\n",
      "Episode = 24\n",
      "t = 219\n",
      "Explore rate: 0.778151\n",
      "Learning rate: 0.778151\n",
      "Streaks: 0\n",
      "Total reward: 0.847917\n",
      "\n",
      "Episode 24 finished after 219.000000 time steps with total reward = 0.847917 (streak 0).\n",
      "\n",
      "Episode = 25\n",
      "t = 153\n",
      "Explore rate: 0.760422\n",
      "Learning rate: 0.760422\n",
      "Streaks: 0\n",
      "Total reward: 0.893750\n",
      "\n",
      "Episode 25 finished after 153.000000 time steps with total reward = 0.893750 (streak 0).\n",
      "\n",
      "Episode = 26\n",
      "t = 206\n",
      "Explore rate: 0.743389\n",
      "Learning rate: 0.743389\n",
      "Streaks: 0\n",
      "Total reward: 0.856944\n",
      "\n",
      "Episode 26 finished after 206.000000 time steps with total reward = 0.856944 (streak 0).\n",
      "\n",
      "Episode = 27\n",
      "t = 136\n",
      "Explore rate: 0.726999\n",
      "Learning rate: 0.726999\n",
      "Streaks: 0\n",
      "Total reward: 0.905556\n",
      "\n",
      "Episode 27 finished after 136.000000 time steps with total reward = 0.905556 (streak 0).\n",
      "\n",
      "Episode = 28\n",
      "t = 291\n",
      "Explore rate: 0.711204\n",
      "Learning rate: 0.711204\n",
      "Streaks: 1\n",
      "Total reward: 0.797917\n",
      "\n",
      "Episode 28 finished after 291.000000 time steps with total reward = 0.797917 (streak 1).\n",
      "\n",
      "Episode = 29\n",
      "t = 122\n",
      "Explore rate: 0.695964\n",
      "Learning rate: 0.695964\n",
      "Streaks: 0\n",
      "Total reward: 0.915278\n",
      "\n",
      "Episode 29 finished after 122.000000 time steps with total reward = 0.915278 (streak 0).\n",
      "\n",
      "Episode = 30\n",
      "t = 132\n",
      "Explore rate: 0.681241\n",
      "Learning rate: 0.681241\n",
      "Streaks: 1\n",
      "Total reward: 0.908333\n",
      "\n",
      "Episode 30 finished after 132.000000 time steps with total reward = 0.908333 (streak 1).\n",
      "\n",
      "Episode = 31\n",
      "t = 137\n",
      "Explore rate: 0.667001\n",
      "Learning rate: 0.667001\n",
      "Streaks: 2\n",
      "Total reward: 0.904861\n",
      "\n",
      "Episode 31 finished after 137.000000 time steps with total reward = 0.904861 (streak 2).\n",
      "\n",
      "Episode = 32\n",
      "t = 125\n",
      "Explore rate: 0.653213\n",
      "Learning rate: 0.653213\n",
      "Streaks: 3\n",
      "Total reward: 0.913194\n",
      "\n",
      "Episode 32 finished after 125.000000 time steps with total reward = 0.913194 (streak 3).\n",
      "\n",
      "Episode = 33\n",
      "t = 142\n",
      "Explore rate: 0.639849\n",
      "Learning rate: 0.639849\n",
      "Streaks: 4\n",
      "Total reward: 0.901389\n",
      "\n",
      "Episode 33 finished after 142.000000 time steps with total reward = 0.901389 (streak 4).\n",
      "\n",
      "Episode = 34\n",
      "t = 106\n",
      "Explore rate: 0.626884\n",
      "Learning rate: 0.626884\n",
      "Streaks: 5\n",
      "Total reward: 0.926389\n",
      "\n",
      "Episode 34 finished after 106.000000 time steps with total reward = 0.926389 (streak 5).\n",
      "\n",
      "Episode = 35\n",
      "t = 124\n",
      "Explore rate: 0.614294\n",
      "Learning rate: 0.614294\n",
      "Streaks: 6\n",
      "Total reward: 0.913889\n",
      "\n",
      "Episode 35 finished after 124.000000 time steps with total reward = 0.913889 (streak 6).\n",
      "\n",
      "Episode = 36\n",
      "t = 108\n",
      "Explore rate: 0.602060\n",
      "Learning rate: 0.602060\n",
      "Streaks: 7\n",
      "Total reward: 0.925000\n",
      "\n",
      "Episode 36 finished after 108.000000 time steps with total reward = 0.925000 (streak 7).\n",
      "\n",
      "Episode = 37\n",
      "t = 113\n",
      "Explore rate: 0.590161\n",
      "Learning rate: 0.590161\n",
      "Streaks: 8\n",
      "Total reward: 0.921528\n",
      "\n",
      "Episode 37 finished after 113.000000 time steps with total reward = 0.921528 (streak 8).\n",
      "\n",
      "Episode = 38\n",
      "t = 130\n",
      "Explore rate: 0.578579\n",
      "Learning rate: 0.578579\n",
      "Streaks: 9\n",
      "Total reward: 0.909722\n",
      "\n",
      "Episode 38 finished after 130.000000 time steps with total reward = 0.909722 (streak 9).\n",
      "\n",
      "Episode = 39\n",
      "t = 94\n",
      "Explore rate: 0.567298\n",
      "Learning rate: 0.567298\n",
      "Streaks: 10\n",
      "Total reward: 0.934722\n",
      "\n",
      "Episode 39 finished after 94.000000 time steps with total reward = 0.934722 (streak 10).\n",
      "\n",
      "Episode = 40\n",
      "t = 124\n",
      "Explore rate: 0.556303\n",
      "Learning rate: 0.556303\n",
      "Streaks: 11\n",
      "Total reward: 0.913889\n",
      "\n",
      "Episode 40 finished after 124.000000 time steps with total reward = 0.913889 (streak 11).\n",
      "\n",
      "Episode = 41\n",
      "t = 86\n",
      "Explore rate: 0.545579\n",
      "Learning rate: 0.545579\n",
      "Streaks: 12\n",
      "Total reward: 0.940278\n",
      "\n",
      "Episode 41 finished after 86.000000 time steps with total reward = 0.940278 (streak 12).\n",
      "\n",
      "Episode = 42\n",
      "t = 103\n",
      "Explore rate: 0.535113\n",
      "Learning rate: 0.535113\n",
      "Streaks: 13\n",
      "Total reward: 0.928472\n",
      "\n",
      "Episode 42 finished after 103.000000 time steps with total reward = 0.928472 (streak 13).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode = 43\n",
      "t = 80\n",
      "Explore rate: 0.524894\n",
      "Learning rate: 0.524894\n",
      "Streaks: 14\n",
      "Total reward: 0.944444\n",
      "\n",
      "Episode 43 finished after 80.000000 time steps with total reward = 0.944444 (streak 14).\n",
      "\n",
      "Episode = 44\n",
      "t = 80\n",
      "Explore rate: 0.514910\n",
      "Learning rate: 0.514910\n",
      "Streaks: 15\n",
      "Total reward: 0.944444\n",
      "\n",
      "Episode 44 finished after 80.000000 time steps with total reward = 0.944444 (streak 15).\n",
      "\n",
      "Episode = 45\n",
      "t = 91\n",
      "Explore rate: 0.505150\n",
      "Learning rate: 0.505150\n",
      "Streaks: 16\n",
      "Total reward: 0.936806\n",
      "\n",
      "Episode 45 finished after 91.000000 time steps with total reward = 0.936806 (streak 16).\n",
      "\n",
      "Episode = 46\n",
      "t = 106\n",
      "Explore rate: 0.495605\n",
      "Learning rate: 0.495605\n",
      "Streaks: 17\n",
      "Total reward: 0.926389\n",
      "\n",
      "Episode 46 finished after 106.000000 time steps with total reward = 0.926389 (streak 17).\n",
      "\n",
      "Episode = 47\n",
      "t = 82\n",
      "Explore rate: 0.486265\n",
      "Learning rate: 0.486265\n",
      "Streaks: 18\n",
      "Total reward: 0.943056\n",
      "\n",
      "Episode 47 finished after 82.000000 time steps with total reward = 0.943056 (streak 18).\n",
      "\n",
      "Episode = 48\n",
      "t = 77\n",
      "Explore rate: 0.477121\n",
      "Learning rate: 0.477121\n",
      "Streaks: 19\n",
      "Total reward: 0.946528\n",
      "\n",
      "Episode 48 finished after 77.000000 time steps with total reward = 0.946528 (streak 19).\n",
      "\n",
      "Episode = 49\n",
      "t = 65\n",
      "Explore rate: 0.468166\n",
      "Learning rate: 0.468166\n",
      "Streaks: 20\n",
      "Total reward: 0.954861\n",
      "\n",
      "Episode 49 finished after 65.000000 time steps with total reward = 0.954861 (streak 20).\n",
      "\n",
      "Episode = 50\n",
      "t = 73\n",
      "Explore rate: 0.459392\n",
      "Learning rate: 0.459392\n",
      "Streaks: 21\n",
      "Total reward: 0.949306\n",
      "\n",
      "Episode 50 finished after 73.000000 time steps with total reward = 0.949306 (streak 21).\n",
      "\n",
      "Episode = 51\n",
      "t = 98\n",
      "Explore rate: 0.450792\n",
      "Learning rate: 0.450792\n",
      "Streaks: 22\n",
      "Total reward: 0.931944\n",
      "\n",
      "Episode 51 finished after 98.000000 time steps with total reward = 0.931944 (streak 22).\n",
      "\n",
      "Episode = 52\n",
      "t = 93\n",
      "Explore rate: 0.442359\n",
      "Learning rate: 0.442359\n",
      "Streaks: 23\n",
      "Total reward: 0.935417\n",
      "\n",
      "Episode 52 finished after 93.000000 time steps with total reward = 0.935417 (streak 23).\n",
      "\n",
      "Episode = 53\n",
      "t = 109\n",
      "Explore rate: 0.434087\n",
      "Learning rate: 0.434087\n",
      "Streaks: 24\n",
      "Total reward: 0.924306\n",
      "\n",
      "Episode 53 finished after 109.000000 time steps with total reward = 0.924306 (streak 24).\n",
      "\n",
      "Episode = 54\n",
      "t = 79\n",
      "Explore rate: 0.425969\n",
      "Learning rate: 0.425969\n",
      "Streaks: 25\n",
      "Total reward: 0.945139\n",
      "\n",
      "Episode 54 finished after 79.000000 time steps with total reward = 0.945139 (streak 25).\n",
      "\n",
      "Episode = 55\n",
      "t = 73\n",
      "Explore rate: 0.418000\n",
      "Learning rate: 0.418000\n",
      "Streaks: 26\n",
      "Total reward: 0.949306\n",
      "\n",
      "Episode 55 finished after 73.000000 time steps with total reward = 0.949306 (streak 26).\n",
      "\n",
      "Episode = 56\n",
      "t = 64\n",
      "Explore rate: 0.410174\n",
      "Learning rate: 0.410174\n",
      "Streaks: 27\n",
      "Total reward: 0.955556\n",
      "\n",
      "Episode 56 finished after 64.000000 time steps with total reward = 0.955556 (streak 27).\n",
      "\n",
      "Episode = 57\n",
      "t = 79\n",
      "Explore rate: 0.402488\n",
      "Learning rate: 0.402488\n",
      "Streaks: 28\n",
      "Total reward: 0.945139\n",
      "\n",
      "Episode 57 finished after 79.000000 time steps with total reward = 0.945139 (streak 28).\n",
      "\n",
      "Episode = 58\n",
      "t = 58\n",
      "Explore rate: 0.394934\n",
      "Learning rate: 0.394934\n",
      "Streaks: 29\n",
      "Total reward: 0.959722\n",
      "\n",
      "Episode 58 finished after 58.000000 time steps with total reward = 0.959722 (streak 29).\n",
      "\n",
      "Episode = 59\n",
      "t = 67\n",
      "Explore rate: 0.387510\n",
      "Learning rate: 0.387510\n",
      "Streaks: 30\n",
      "Total reward: 0.953472\n",
      "\n",
      "Episode 59 finished after 67.000000 time steps with total reward = 0.953472 (streak 30).\n",
      "\n",
      "Episode = 60\n",
      "t = 74\n",
      "Explore rate: 0.380211\n",
      "Learning rate: 0.380211\n",
      "Streaks: 31\n",
      "Total reward: 0.948611\n",
      "\n",
      "Episode 60 finished after 74.000000 time steps with total reward = 0.948611 (streak 31).\n",
      "\n",
      "Episode = 61\n",
      "t = 55\n",
      "Explore rate: 0.373033\n",
      "Learning rate: 0.373033\n",
      "Streaks: 32\n",
      "Total reward: 0.961806\n",
      "\n",
      "Episode 61 finished after 55.000000 time steps with total reward = 0.961806 (streak 32).\n",
      "\n",
      "Episode = 62\n",
      "t = 69\n",
      "Explore rate: 0.365971\n",
      "Learning rate: 0.365971\n",
      "Streaks: 33\n",
      "Total reward: 0.952083\n",
      "\n",
      "Episode 62 finished after 69.000000 time steps with total reward = 0.952083 (streak 33).\n",
      "\n",
      "Episode = 63\n",
      "t = 75\n",
      "Explore rate: 0.359022\n",
      "Learning rate: 0.359022\n",
      "Streaks: 34\n",
      "Total reward: 0.947917\n",
      "\n",
      "Episode 63 finished after 75.000000 time steps with total reward = 0.947917 (streak 34).\n",
      "\n",
      "Episode = 64\n",
      "t = 67\n",
      "Explore rate: 0.352183\n",
      "Learning rate: 0.352183\n",
      "Streaks: 35\n",
      "Total reward: 0.953472\n",
      "\n",
      "Episode 64 finished after 67.000000 time steps with total reward = 0.953472 (streak 35).\n",
      "\n",
      "Episode = 65\n",
      "t = 64\n",
      "Explore rate: 0.345449\n",
      "Learning rate: 0.345449\n",
      "Streaks: 36\n",
      "Total reward: 0.955556\n",
      "\n",
      "Episode 65 finished after 64.000000 time steps with total reward = 0.955556 (streak 36).\n",
      "\n",
      "Episode = 66\n",
      "t = 81\n",
      "Explore rate: 0.338819\n",
      "Learning rate: 0.338819\n",
      "Streaks: 37\n",
      "Total reward: 0.943750\n",
      "\n",
      "Episode 66 finished after 81.000000 time steps with total reward = 0.943750 (streak 37).\n",
      "\n",
      "Episode = 67\n",
      "t = 60\n",
      "Explore rate: 0.332288\n",
      "Learning rate: 0.332288\n",
      "Streaks: 38\n",
      "Total reward: 0.958333\n",
      "\n",
      "Episode 67 finished after 60.000000 time steps with total reward = 0.958333 (streak 38).\n",
      "\n",
      "Episode = 68\n",
      "t = 63\n",
      "Explore rate: 0.325854\n",
      "Learning rate: 0.325854\n",
      "Streaks: 39\n",
      "Total reward: 0.956250\n",
      "\n",
      "Episode 68 finished after 63.000000 time steps with total reward = 0.956250 (streak 39).\n",
      "\n",
      "Episode = 69\n",
      "t = 59\n",
      "Explore rate: 0.319513\n",
      "Learning rate: 0.319513\n",
      "Streaks: 40\n",
      "Total reward: 0.959028\n",
      "\n",
      "Episode 69 finished after 59.000000 time steps with total reward = 0.959028 (streak 40).\n",
      "\n",
      "Episode = 70\n",
      "t = 73\n",
      "Explore rate: 0.313264\n",
      "Learning rate: 0.313264\n",
      "Streaks: 41\n",
      "Total reward: 0.949306\n",
      "\n",
      "Episode 70 finished after 73.000000 time steps with total reward = 0.949306 (streak 41).\n",
      "\n",
      "Episode = 71\n",
      "t = 62\n",
      "Explore rate: 0.307104\n",
      "Learning rate: 0.307104\n",
      "Streaks: 42\n",
      "Total reward: 0.956944\n",
      "\n",
      "Episode 71 finished after 62.000000 time steps with total reward = 0.956944 (streak 42).\n",
      "\n",
      "Episode = 72\n",
      "t = 61\n",
      "Explore rate: 0.301030\n",
      "Learning rate: 0.301030\n",
      "Streaks: 43\n",
      "Total reward: 0.957639\n",
      "\n",
      "Episode 72 finished after 61.000000 time steps with total reward = 0.957639 (streak 43).\n",
      "\n",
      "Episode = 73\n",
      "t = 48\n",
      "Explore rate: 0.295040\n",
      "Learning rate: 0.295040\n",
      "Streaks: 44\n",
      "Total reward: 0.966667\n",
      "\n",
      "Episode 73 finished after 48.000000 time steps with total reward = 0.966667 (streak 44).\n",
      "\n",
      "Episode = 74\n",
      "t = 68\n",
      "Explore rate: 0.289131\n",
      "Learning rate: 0.289131\n",
      "Streaks: 45\n",
      "Total reward: 0.952778\n",
      "\n",
      "Episode 74 finished after 68.000000 time steps with total reward = 0.952778 (streak 45).\n",
      "\n",
      "Episode = 75\n",
      "t = 77\n",
      "Explore rate: 0.283301\n",
      "Learning rate: 0.283301\n",
      "Streaks: 46\n",
      "Total reward: 0.946528\n",
      "\n",
      "Episode 75 finished after 77.000000 time steps with total reward = 0.946528 (streak 46).\n",
      "\n",
      "Episode = 76\n",
      "t = 49\n",
      "Explore rate: 0.277549\n",
      "Learning rate: 0.277549\n",
      "Streaks: 47\n",
      "Total reward: 0.965972\n",
      "\n",
      "Episode 76 finished after 49.000000 time steps with total reward = 0.965972 (streak 47).\n",
      "\n",
      "Episode = 77\n",
      "t = 60\n",
      "Explore rate: 0.271872\n",
      "Learning rate: 0.271872\n",
      "Streaks: 48\n",
      "Total reward: 0.958333\n",
      "\n",
      "Episode 77 finished after 60.000000 time steps with total reward = 0.958333 (streak 48).\n",
      "\n",
      "Episode = 78\n",
      "t = 64\n",
      "Explore rate: 0.266268\n",
      "Learning rate: 0.266268\n",
      "Streaks: 49\n",
      "Total reward: 0.955556\n",
      "\n",
      "Episode 78 finished after 64.000000 time steps with total reward = 0.955556 (streak 49).\n",
      "\n",
      "Episode = 79\n",
      "t = 51\n",
      "Explore rate: 0.260735\n",
      "Learning rate: 0.260735\n",
      "Streaks: 50\n",
      "Total reward: 0.964583\n",
      "\n",
      "Episode 79 finished after 51.000000 time steps with total reward = 0.964583 (streak 50).\n",
      "\n",
      "Episode = 80\n",
      "t = 53\n",
      "Explore rate: 0.255273\n",
      "Learning rate: 0.255273\n",
      "Streaks: 51\n",
      "Total reward: 0.963194\n",
      "\n",
      "Episode 80 finished after 53.000000 time steps with total reward = 0.963194 (streak 51).\n",
      "\n",
      "Episode = 81\n",
      "t = 57\n",
      "Explore rate: 0.249877\n",
      "Learning rate: 0.249877\n",
      "Streaks: 52\n",
      "Total reward: 0.960417\n",
      "\n",
      "Episode 81 finished after 57.000000 time steps with total reward = 0.960417 (streak 52).\n",
      "\n",
      "Episode = 82\n",
      "t = 49\n",
      "Explore rate: 0.244549\n",
      "Learning rate: 0.244549\n",
      "Streaks: 53\n",
      "Total reward: 0.965972\n",
      "\n",
      "Episode 82 finished after 49.000000 time steps with total reward = 0.965972 (streak 53).\n",
      "\n",
      "Episode = 83\n",
      "t = 57\n",
      "Explore rate: 0.239284\n",
      "Learning rate: 0.239284\n",
      "Streaks: 54\n",
      "Total reward: 0.960417\n",
      "\n",
      "Episode 83 finished after 57.000000 time steps with total reward = 0.960417 (streak 54).\n",
      "\n",
      "Episode = 84\n",
      "t = 56\n",
      "Explore rate: 0.234083\n",
      "Learning rate: 0.234083\n",
      "Streaks: 55\n",
      "Total reward: 0.961111\n",
      "\n",
      "Episode 84 finished after 56.000000 time steps with total reward = 0.961111 (streak 55).\n",
      "\n",
      "Episode = 85\n",
      "t = 51\n",
      "Explore rate: 0.228944\n",
      "Learning rate: 0.228944\n",
      "Streaks: 56\n",
      "Total reward: 0.964583\n",
      "\n",
      "Episode 85 finished after 51.000000 time steps with total reward = 0.964583 (streak 56).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode = 86\n",
      "t = 49\n",
      "Explore rate: 0.223864\n",
      "Learning rate: 0.223864\n",
      "Streaks: 57\n",
      "Total reward: 0.965972\n",
      "\n",
      "Episode 86 finished after 49.000000 time steps with total reward = 0.965972 (streak 57).\n",
      "\n",
      "Episode = 87\n",
      "t = 50\n",
      "Explore rate: 0.218843\n",
      "Learning rate: 0.218843\n",
      "Streaks: 58\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 87 finished after 50.000000 time steps with total reward = 0.965278 (streak 58).\n",
      "\n",
      "Episode = 88\n",
      "t = 53\n",
      "Explore rate: 0.213880\n",
      "Learning rate: 0.213880\n",
      "Streaks: 59\n",
      "Total reward: 0.963194\n",
      "\n",
      "Episode 88 finished after 53.000000 time steps with total reward = 0.963194 (streak 59).\n",
      "\n",
      "Episode = 89\n",
      "t = 47\n",
      "Explore rate: 0.208972\n",
      "Learning rate: 0.208972\n",
      "Streaks: 60\n",
      "Total reward: 0.967361\n",
      "\n",
      "Episode 89 finished after 47.000000 time steps with total reward = 0.967361 (streak 60).\n",
      "\n",
      "Episode = 90\n",
      "t = 51\n",
      "Explore rate: 0.204120\n",
      "Learning rate: 0.204120\n",
      "Streaks: 61\n",
      "Total reward: 0.964583\n",
      "\n",
      "Episode 90 finished after 51.000000 time steps with total reward = 0.964583 (streak 61).\n",
      "\n",
      "Episode = 91\n",
      "t = 55\n",
      "Explore rate: 0.199321\n",
      "Learning rate: 0.200000\n",
      "Streaks: 62\n",
      "Total reward: 0.961806\n",
      "\n",
      "Episode 91 finished after 55.000000 time steps with total reward = 0.961806 (streak 62).\n",
      "\n",
      "Episode = 92\n",
      "t = 50\n",
      "Explore rate: 0.194575\n",
      "Learning rate: 0.200000\n",
      "Streaks: 63\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 92 finished after 50.000000 time steps with total reward = 0.965278 (streak 63).\n",
      "\n",
      "Episode = 93\n",
      "t = 50\n",
      "Explore rate: 0.189880\n",
      "Learning rate: 0.200000\n",
      "Streaks: 64\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 93 finished after 50.000000 time steps with total reward = 0.965278 (streak 64).\n",
      "\n",
      "Episode = 94\n",
      "t = 50\n",
      "Explore rate: 0.185235\n",
      "Learning rate: 0.200000\n",
      "Streaks: 65\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 94 finished after 50.000000 time steps with total reward = 0.965278 (streak 65).\n",
      "\n",
      "Episode = 95\n",
      "t = 51\n",
      "Explore rate: 0.180639\n",
      "Learning rate: 0.200000\n",
      "Streaks: 66\n",
      "Total reward: 0.964583\n",
      "\n",
      "Episode 95 finished after 51.000000 time steps with total reward = 0.964583 (streak 66).\n",
      "\n",
      "Episode = 96\n",
      "t = 52\n",
      "Explore rate: 0.176091\n",
      "Learning rate: 0.200000\n",
      "Streaks: 67\n",
      "Total reward: 0.963889\n",
      "\n",
      "Episode 96 finished after 52.000000 time steps with total reward = 0.963889 (streak 67).\n",
      "\n",
      "Episode = 97\n",
      "t = 44\n",
      "Explore rate: 0.171591\n",
      "Learning rate: 0.200000\n",
      "Streaks: 68\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 97 finished after 44.000000 time steps with total reward = 0.969444 (streak 68).\n",
      "\n",
      "Episode = 98\n",
      "t = 45\n",
      "Explore rate: 0.167136\n",
      "Learning rate: 0.200000\n",
      "Streaks: 69\n",
      "Total reward: 0.968750\n",
      "\n",
      "Episode 98 finished after 45.000000 time steps with total reward = 0.968750 (streak 69).\n",
      "\n",
      "Episode = 99\n",
      "t = 50\n",
      "Explore rate: 0.162727\n",
      "Learning rate: 0.200000\n",
      "Streaks: 70\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 99 finished after 50.000000 time steps with total reward = 0.965278 (streak 70).\n",
      "\n",
      "Episode = 100\n",
      "t = 50\n",
      "Explore rate: 0.158362\n",
      "Learning rate: 0.200000\n",
      "Streaks: 71\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 100 finished after 50.000000 time steps with total reward = 0.965278 (streak 71).\n",
      "\n",
      "Episode = 101\n",
      "t = 45\n",
      "Explore rate: 0.154041\n",
      "Learning rate: 0.200000\n",
      "Streaks: 72\n",
      "Total reward: 0.968750\n",
      "\n",
      "Episode 101 finished after 45.000000 time steps with total reward = 0.968750 (streak 72).\n",
      "\n",
      "Episode = 102\n",
      "t = 50\n",
      "Explore rate: 0.149762\n",
      "Learning rate: 0.200000\n",
      "Streaks: 73\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 102 finished after 50.000000 time steps with total reward = 0.965278 (streak 73).\n",
      "\n",
      "Episode = 103\n",
      "t = 57\n",
      "Explore rate: 0.145525\n",
      "Learning rate: 0.200000\n",
      "Streaks: 74\n",
      "Total reward: 0.960417\n",
      "\n",
      "Episode 103 finished after 57.000000 time steps with total reward = 0.960417 (streak 74).\n",
      "\n",
      "Episode = 104\n",
      "t = 54\n",
      "Explore rate: 0.141329\n",
      "Learning rate: 0.200000\n",
      "Streaks: 75\n",
      "Total reward: 0.962500\n",
      "\n",
      "Episode 104 finished after 54.000000 time steps with total reward = 0.962500 (streak 75).\n",
      "\n",
      "Episode = 105\n",
      "t = 48\n",
      "Explore rate: 0.137173\n",
      "Learning rate: 0.200000\n",
      "Streaks: 76\n",
      "Total reward: 0.966667\n",
      "\n",
      "Episode 105 finished after 48.000000 time steps with total reward = 0.966667 (streak 76).\n",
      "\n",
      "Episode = 106\n",
      "t = 44\n",
      "Explore rate: 0.133057\n",
      "Learning rate: 0.200000\n",
      "Streaks: 77\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 106 finished after 44.000000 time steps with total reward = 0.969444 (streak 77).\n",
      "\n",
      "Episode = 107\n",
      "t = 47\n",
      "Explore rate: 0.128979\n",
      "Learning rate: 0.200000\n",
      "Streaks: 78\n",
      "Total reward: 0.967361\n",
      "\n",
      "Episode 107 finished after 47.000000 time steps with total reward = 0.967361 (streak 78).\n",
      "\n",
      "Episode = 108\n",
      "t = 44\n",
      "Explore rate: 0.124939\n",
      "Learning rate: 0.200000\n",
      "Streaks: 79\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 108 finished after 44.000000 time steps with total reward = 0.969444 (streak 79).\n",
      "\n",
      "Episode = 109\n",
      "t = 50\n",
      "Explore rate: 0.120936\n",
      "Learning rate: 0.200000\n",
      "Streaks: 80\n",
      "Total reward: 0.965278\n",
      "\n",
      "Episode 109 finished after 50.000000 time steps with total reward = 0.965278 (streak 80).\n",
      "\n",
      "Episode = 110\n",
      "t = 49\n",
      "Explore rate: 0.116970\n",
      "Learning rate: 0.200000\n",
      "Streaks: 81\n",
      "Total reward: 0.965972\n",
      "\n",
      "Episode 110 finished after 49.000000 time steps with total reward = 0.965972 (streak 81).\n",
      "\n",
      "Episode = 111\n",
      "t = 45\n",
      "Explore rate: 0.113040\n",
      "Learning rate: 0.200000\n",
      "Streaks: 82\n",
      "Total reward: 0.968750\n",
      "\n",
      "Episode 111 finished after 45.000000 time steps with total reward = 0.968750 (streak 82).\n",
      "\n",
      "Episode = 112\n",
      "t = 44\n",
      "Explore rate: 0.109144\n",
      "Learning rate: 0.200000\n",
      "Streaks: 83\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 112 finished after 44.000000 time steps with total reward = 0.969444 (streak 83).\n",
      "\n",
      "Episode = 113\n",
      "t = 44\n",
      "Explore rate: 0.105284\n",
      "Learning rate: 0.200000\n",
      "Streaks: 84\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 113 finished after 44.000000 time steps with total reward = 0.969444 (streak 84).\n",
      "\n",
      "Episode = 114\n",
      "t = 42\n",
      "Explore rate: 0.101458\n",
      "Learning rate: 0.200000\n",
      "Streaks: 85\n",
      "Total reward: 0.970833\n",
      "\n",
      "Episode 114 finished after 42.000000 time steps with total reward = 0.970833 (streak 85).\n",
      "\n",
      "Episode = 115\n",
      "t = 49\n",
      "Explore rate: 0.097665\n",
      "Learning rate: 0.200000\n",
      "Streaks: 86\n",
      "Total reward: 0.965972\n",
      "\n",
      "Episode 115 finished after 49.000000 time steps with total reward = 0.965972 (streak 86).\n",
      "\n",
      "Episode = 116\n",
      "t = 45\n",
      "Explore rate: 0.093905\n",
      "Learning rate: 0.200000\n",
      "Streaks: 87\n",
      "Total reward: 0.968750\n",
      "\n",
      "Episode 116 finished after 45.000000 time steps with total reward = 0.968750 (streak 87).\n",
      "\n",
      "Episode = 117\n",
      "t = 45\n",
      "Explore rate: 0.090177\n",
      "Learning rate: 0.200000\n",
      "Streaks: 88\n",
      "Total reward: 0.968750\n",
      "\n",
      "Episode 117 finished after 45.000000 time steps with total reward = 0.968750 (streak 88).\n",
      "\n",
      "Episode = 118\n",
      "t = 43\n",
      "Explore rate: 0.086480\n",
      "Learning rate: 0.200000\n",
      "Streaks: 89\n",
      "Total reward: 0.970139\n",
      "\n",
      "Episode 118 finished after 43.000000 time steps with total reward = 0.970139 (streak 89).\n",
      "\n",
      "Episode = 119\n",
      "t = 44\n",
      "Explore rate: 0.082816\n",
      "Learning rate: 0.200000\n",
      "Streaks: 90\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 119 finished after 44.000000 time steps with total reward = 0.969444 (streak 90).\n",
      "\n",
      "Episode = 120\n",
      "t = 43\n",
      "Explore rate: 0.079181\n",
      "Learning rate: 0.200000\n",
      "Streaks: 91\n",
      "Total reward: 0.970139\n",
      "\n",
      "Episode 120 finished after 43.000000 time steps with total reward = 0.970139 (streak 91).\n",
      "\n",
      "Episode = 121\n",
      "t = 42\n",
      "Explore rate: 0.075577\n",
      "Learning rate: 0.200000\n",
      "Streaks: 92\n",
      "Total reward: 0.970833\n",
      "\n",
      "Episode 121 finished after 42.000000 time steps with total reward = 0.970833 (streak 92).\n",
      "\n",
      "Episode = 122\n",
      "t = 46\n",
      "Explore rate: 0.072003\n",
      "Learning rate: 0.200000\n",
      "Streaks: 93\n",
      "Total reward: 0.968056\n",
      "\n",
      "Episode 122 finished after 46.000000 time steps with total reward = 0.968056 (streak 93).\n",
      "\n",
      "Episode = 123\n",
      "t = 41\n",
      "Explore rate: 0.068457\n",
      "Learning rate: 0.200000\n",
      "Streaks: 94\n",
      "Total reward: 0.971528\n",
      "\n",
      "Episode 123 finished after 41.000000 time steps with total reward = 0.971528 (streak 94).\n",
      "\n",
      "Episode = 124\n",
      "t = 43\n",
      "Explore rate: 0.064941\n",
      "Learning rate: 0.200000\n",
      "Streaks: 95\n",
      "Total reward: 0.970139\n",
      "\n",
      "Episode 124 finished after 43.000000 time steps with total reward = 0.970139 (streak 95).\n",
      "\n",
      "Episode = 125\n",
      "t = 44\n",
      "Explore rate: 0.061452\n",
      "Learning rate: 0.200000\n",
      "Streaks: 96\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 125 finished after 44.000000 time steps with total reward = 0.969444 (streak 96).\n",
      "\n",
      "Episode = 126\n",
      "t = 46\n",
      "Explore rate: 0.057992\n",
      "Learning rate: 0.200000\n",
      "Streaks: 97\n",
      "Total reward: 0.968056\n",
      "\n",
      "Episode 126 finished after 46.000000 time steps with total reward = 0.968056 (streak 97).\n",
      "\n",
      "Episode = 127\n",
      "t = 44\n",
      "Explore rate: 0.054559\n",
      "Learning rate: 0.200000\n",
      "Streaks: 98\n",
      "Total reward: 0.969444\n",
      "\n",
      "Episode 127 finished after 44.000000 time steps with total reward = 0.969444 (streak 98).\n",
      "\n",
      "Episode = 128\n",
      "t = 42\n",
      "Explore rate: 0.051153\n",
      "Learning rate: 0.200000\n",
      "Streaks: 99\n",
      "Total reward: 0.970833\n",
      "\n",
      "Episode 128 finished after 42.000000 time steps with total reward = 0.970833 (streak 99).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode = 129\n",
      "t = 41\n",
      "Explore rate: 0.047773\n",
      "Learning rate: 0.200000\n",
      "Streaks: 100\n",
      "Total reward: 0.971528\n",
      "\n",
      "Episode 129 finished after 41.000000 time steps with total reward = 0.971528 (streak 100).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ff64ca52aa7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mENABLE_RECORDING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#env.monitor.close()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Creating a Q-Table for each state-action pair\n",
    "'''\n",
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,), dtype=float)\n",
    "\n",
    "'''\n",
    "Begin simulation\n",
    "'''\n",
    "\n",
    "\n",
    "if ENABLE_RECORDING:\n",
    "    env._start(recording_folder, video_callable=lambda episode: True,force=True)\n",
    "    #env.monitor.start(recording_folder, force=True)\n",
    "\n",
    "simulate(q_table, env)\n",
    "\n",
    "if ENABLE_RECORDING:\n",
    "    env.close()\n",
    "    #env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Video\n",
    "from base64 import b64encode\n",
    "video = open(\"videos/openaigym.video.1.26037.video000000.mp4\", \"rb\").read()\n",
    "video_encoded = b64encode(video).decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  >\n",
       " <source src=\"data:None;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAAhtZGF0AAAA1m1vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAAAAAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAABidWR0YQAAAFptZXRhAAAAAAAAACFoZGxyAAAAAAAAAABtZGlyYXBwbAAAAAAAAAAAAAAAAC1pbHN0AAAAJal0b28AAAAdZGF0YQAAAAEAAAAATGF2ZjU3LjgzLjEwMA==\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(video_encoded, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['videos/openaigym.video.1.23139.video000000.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000001.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000002.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000003.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000004.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000005.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000006.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000007.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000008.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000009.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000010.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000011.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000012.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000013.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000014.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000015.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000016.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000017.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000018.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000019.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000020.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000021.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000022.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000023.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000024.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000025.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000026.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000027.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000028.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000029.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000030.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000031.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000032.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000033.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000034.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000035.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000036.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000037.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000038.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000039.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000040.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000041.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000042.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000043.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000044.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000045.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000046.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000047.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000048.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000049.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000050.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000051.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000052.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000053.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000054.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000055.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000056.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000057.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000058.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000059.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000060.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000061.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000062.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000063.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000064.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000065.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000066.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000067.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000068.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000069.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000070.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000071.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000072.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000073.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000074.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000075.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000076.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000077.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000078.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000079.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000080.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000081.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000082.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000083.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000084.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000085.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000086.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000087.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000088.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000089.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000090.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000091.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000092.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000093.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000094.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000095.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000096.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000097.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000098.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000099.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000100.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000101.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000102.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000103.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000104.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000105.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000106.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000107.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000108.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000109.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000110.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000111.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000112.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000113.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000114.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000115.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000116.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000117.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000118.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000119.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000120.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000121.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000122.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000124.mp4',\n",
       " 'videos/openaigym.video.1.23139.video000123.mp4']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = sorted(glob('videos/*.mp4'), key=os.path.getmtime)\n",
    "filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce2f8b43e624518a1ccb40731e610bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Output(layout=Layout(grid_area='widget001')), Output(layout=Layout(grid_area='widget0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output, GridspecLayout\n",
    "from IPython import display\n",
    "\n",
    "grid = GridspecLayout(1, len(filepaths))\n",
    "\n",
    "for i, filepath in enumerate(filepaths):\n",
    "    out = Output()\n",
    "    with out:\n",
    "        video = open(filepath, \"rb\").read()\n",
    "        video_encoded = b64encode(video).decode('ascii')\n",
    "        display.display(display.Video(video_encoded, embed=True))\n",
    "        #display.display(display.Video(filepath, embed=True))\n",
    "    grid[0, i] = out\n",
    "\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
